; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=aarch64 < %s | FileCheck %s

; Test that UADDLV results stay in SIMD registers through scalar operations
; and stores, avoiding expensive cross-register-bank moves (fmov).
; Pattern: UADDLV -> extract lane 0 -> [scalar ops] -> [trunc] -> store
; Optimized: UADDLV -> [vector ops on lane 0] -> store from FPR

; ============================================================================
; i32 store tests
; ============================================================================

define void @test_i32_add_srl(<8 x i16> %v0, ptr %out) {
; CHECK-LABEL: test_i32_add_srl:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uaddlv s0, v0.8h
; CHECK-NEXT:    fmov w8, s0
; CHECK-NEXT:    add w8, w8, #32
; CHECK-NEXT:    lsr w8, w8, #6
; CHECK-NEXT:    str w8, [x0]
; CHECK-NEXT:    ret
  %uaddlv = call i32 @llvm.aarch64.neon.uaddlv.i32.v8i16(<8 x i16> %v0)
  %add = add i32 %uaddlv, 32
  %shr = lshr i32 %add, 6
  store i32 %shr, ptr %out, align 4
  ret void
}

define void @test_i32_sub(<8 x i16> %v0, ptr %out) {
; CHECK-LABEL: test_i32_sub:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uaddlv s0, v0.8h
; CHECK-NEXT:    fmov w8, s0
; CHECK-NEXT:    sub w8, w8, #10
; CHECK-NEXT:    str w8, [x0]
; CHECK-NEXT:    ret
  %uaddlv = call i32 @llvm.aarch64.neon.uaddlv.i32.v8i16(<8 x i16> %v0)
  %sub = sub i32 %uaddlv, 10
  store i32 %sub, ptr %out, align 4
  ret void
}

define void @test_i32_and(<8 x i16> %v0, ptr %out) {
; CHECK-LABEL: test_i32_and:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uaddlv s0, v0.8h
; CHECK-NEXT:    fmov w8, s0
; CHECK-NEXT:    and w8, w8, #0xff
; CHECK-NEXT:    str w8, [x0]
; CHECK-NEXT:    ret
  %uaddlv = call i32 @llvm.aarch64.neon.uaddlv.i32.v8i16(<8 x i16> %v0)
  %and = and i32 %uaddlv, 255
  store i32 %and, ptr %out, align 4
  ret void
}

define void @test_i32_or(<8 x i16> %v0, ptr %out) {
; CHECK-LABEL: test_i32_or:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uaddlv s0, v0.8h
; CHECK-NEXT:    fmov w8, s0
; CHECK-NEXT:    orr w8, w8, #0x80
; CHECK-NEXT:    str w8, [x0]
; CHECK-NEXT:    ret
  %uaddlv = call i32 @llvm.aarch64.neon.uaddlv.i32.v8i16(<8 x i16> %v0)
  %or = or i32 %uaddlv, 128
  store i32 %or, ptr %out, align 4
  ret void
}

define void @test_i32_xor(<8 x i16> %v0, ptr %out) {
; CHECK-LABEL: test_i32_xor:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uaddlv s0, v0.8h
; CHECK-NEXT:    mov w8, #42 // =0x2a
; CHECK-NEXT:    fmov w9, s0
; CHECK-NEXT:    eor w8, w9, w8
; CHECK-NEXT:    str w8, [x0]
; CHECK-NEXT:    ret
  %uaddlv = call i32 @llvm.aarch64.neon.uaddlv.i32.v8i16(<8 x i16> %v0)
  %xor = xor i32 %uaddlv, 42
  store i32 %xor, ptr %out, align 4
  ret void
}

define void @test_i32_mul(<8 x i16> %v0, ptr %out) {
; CHECK-LABEL: test_i32_mul:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uaddlv s0, v0.8h
; CHECK-NEXT:    fmov w8, s0
; CHECK-NEXT:    add w8, w8, w8, lsl #1
; CHECK-NEXT:    str w8, [x0]
; CHECK-NEXT:    ret
  %uaddlv = call i32 @llvm.aarch64.neon.uaddlv.i32.v8i16(<8 x i16> %v0)
  %mul = mul i32 %uaddlv, 3
  store i32 %mul, ptr %out, align 4
  ret void
}

define void @test_i32_shl(<8 x i16> %v0, ptr %out) {
; CHECK-LABEL: test_i32_shl:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uaddlv s0, v0.8h
; CHECK-NEXT:    fmov w8, s0
; CHECK-NEXT:    lsl w8, w8, #2
; CHECK-NEXT:    str w8, [x0]
; CHECK-NEXT:    ret
  %uaddlv = call i32 @llvm.aarch64.neon.uaddlv.i32.v8i16(<8 x i16> %v0)
  %shl = shl i32 %uaddlv, 2
  store i32 %shl, ptr %out, align 4
  ret void
}

; Test chaining multiple operations
define void @test_i32_chain(<8 x i16> %v0, ptr %out) {
; CHECK-LABEL: test_i32_chain:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uaddlv s0, v0.8h
; CHECK-NEXT:    fmov w8, s0
; CHECK-NEXT:    add w8, w8, #32
; CHECK-NEXT:    ubfx w8, w8, #2, #8
; CHECK-NEXT:    str w8, [x0]
; CHECK-NEXT:    ret
  %uaddlv = call i32 @llvm.aarch64.neon.uaddlv.i32.v8i16(<8 x i16> %v0)
  %add = add i32 %uaddlv, 32
  %and = and i32 %add, 1023
  %shr = lshr i32 %and, 2
  store i32 %shr, ptr %out, align 4
  ret void
}

; ============================================================================
; i16 truncating store tests
; ============================================================================

define void @test_i16_trunc_store(<8 x i16> %v0, ptr %out) {
; CHECK-LABEL: test_i16_trunc_store:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uaddlv s0, v0.8h
; CHECK-NEXT:    fmov w8, s0
; CHECK-NEXT:    lsr w8, w8, #2
; CHECK-NEXT:    strh w8, [x0]
; CHECK-NEXT:    ret
  %uaddlv = call i32 @llvm.aarch64.neon.uaddlv.i32.v8i16(<8 x i16> %v0)
  %shr = lshr i32 %uaddlv, 2
  %trunc = trunc i32 %shr to i16
  store i16 %trunc, ptr %out, align 2
  ret void
}

define void @test_i16_ops(<8 x i16> %v0, ptr %out) {
; CHECK-LABEL: test_i16_ops:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uaddlv s0, v0.8h
; CHECK-NEXT:    fmov w8, s0
; CHECK-NEXT:    add w8, w8, w8, lsl #1
; CHECK-NEXT:    add w8, w8, #300
; CHECK-NEXT:    strh w8, [x0]
; CHECK-NEXT:    ret
  %uaddlv = call i32 @llvm.aarch64.neon.uaddlv.i32.v8i16(<8 x i16> %v0)
  %add = add i32 %uaddlv, 100
  %mul = mul i32 %add, 3
  %and = and i32 %mul, 65535
  %trunc = trunc i32 %and to i16
  store i16 %trunc, ptr %out, align 2
  ret void
}

; ============================================================================
; i8 truncating store tests
; ============================================================================

define void @test_i8_trunc_store(<8 x i16> %v0, ptr %out) {
; CHECK-LABEL: test_i8_trunc_store:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uaddlv s0, v0.8h
; CHECK-NEXT:    fmov w8, s0
; CHECK-NEXT:    add w8, w8, #10
; CHECK-NEXT:    strb w8, [x0]
; CHECK-NEXT:    ret
  %uaddlv = call i32 @llvm.aarch64.neon.uaddlv.i32.v8i16(<8 x i16> %v0)
  %add = add i32 %uaddlv, 10
  %trunc = trunc i32 %add to i8
  store i8 %trunc, ptr %out, align 1
  ret void
}

define void @test_i8_ops(<8 x i16> %v0, ptr %out) {
; CHECK-LABEL: test_i8_ops:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uaddlv s0, v0.8h
; CHECK-NEXT:    fmov w8, s0
; CHECK-NEXT:    add w8, w8, #32
; CHECK-NEXT:    ubfx w8, w8, #2, #6
; CHECK-NEXT:    strb w8, [x0]
; CHECK-NEXT:    ret
  %uaddlv = call i32 @llvm.aarch64.neon.uaddlv.i32.v8i16(<8 x i16> %v0)
  %add = add i32 %uaddlv, 32
  %and = and i32 %add, 255
  %shr = lshr i32 %and, 2
  %trunc = trunc i32 %shr to i8
  store i8 %trunc, ptr %out, align 1
  ret void
}

; ============================================================================
; i64 store tests
; ============================================================================

define void @test_i64_add(<4 x i32> %v0, ptr %out) {
; CHECK-LABEL: test_i64_add:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uaddlv d0, v0.4s
; CHECK-NEXT:    fmov x8, d0
; CHECK-NEXT:    add x8, x8, #100
; CHECK-NEXT:    str x8, [x0]
; CHECK-NEXT:    ret
  %uaddlv = call i64 @llvm.aarch64.neon.uaddlv.i64.v4i32(<4 x i32> %v0)
  %add = add i64 %uaddlv, 100
  store i64 %add, ptr %out, align 8
  ret void
}

define void @test_i64_ops(<4 x i32> %v0, ptr %out) {
; CHECK-LABEL: test_i64_ops:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uaddlv d0, v0.4s
; CHECK-NEXT:    fmov x8, d0
; CHECK-NEXT:    add x8, x8, #50
; CHECK-NEXT:    ubfx x8, x8, #3, #32
; CHECK-NEXT:    str x8, [x0]
; CHECK-NEXT:    ret
  %uaddlv = call i64 @llvm.aarch64.neon.uaddlv.i64.v4i32(<4 x i32> %v0)
  %add = add i64 %uaddlv, 50
  %shr = lshr i64 %add, 3
  %and = and i64 %shr, 4294967295
  store i64 %and, ptr %out, align 8
  ret void
}

declare i32 @llvm.aarch64.neon.uaddlv.i32.v8i16(<8 x i16>)
declare i64 @llvm.aarch64.neon.uaddlv.i64.v4i32(<4 x i32>)
